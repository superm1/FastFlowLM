<p align="center">
  <a href="https://www.fastflowlm.com" target="_blank">
    <img src="assets/logo.png" alt="FastFlowLM Logo" width="200"/>
  </a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/NPU-Optimized-red" />
</p>

## ‚ö° FastFlowLM (FLM) ‚Äî Unlock Ryzen‚Ñ¢ AI NPUs

Run large language models ‚Äî now with **Vision** and **MoE** support ‚Äî on **AMD Ryzen‚Ñ¢ AI NPUs** in minutes.  
**No GPU required. Faster and over 10√ó more power-efficient. Supports context lengths up to 256k tokens. Ultra-Lightweight (14 MB). Installs within 20 seconds.**

üì¶ **The only out-of-box, NPU-first runtime built exclusively for Ryzen‚Ñ¢ AI.**  
ü§ù **Think Ollama ‚Äî but deeply optimized for NPUs.**  
‚ú® **From Idle Silicon to Instant Power ‚Äî FastFlowLM Makes Ryzen‚Ñ¢ AI Shine.**

> FastFlowLM (FLM) supports all Ryzen‚Ñ¢ AI Series chips with XDNA2 NPUs (Strix, Strix Halo, and Kraken).

---

## üîó Quick Links

  üîΩ **[Download](https://github.com/FastFlowLM/FastFlowLM/releases/latest/download/flm-setup.exe)** | üìä **[Benchmarks](https://docs.fastflowlm.com/benchmarks/)** | üì¶ **[Model List](https://docs.fastflowlm.com/models/)**  


  üìñ **[Docs](https://docs.fastflowlm.com)** | üì∫ **[Demos](https://www.youtube.com/@FastFlowLM-YT/playlists)** | üß™ **[Test Drive](https://docs.fastflowlm.com/#-test-drive-remote-demo)** | üí¨ **[Discord](https://discord.gg/z24t23HsHF)** 

---

## üöÄ Quick Start

A packaged FLM Windows installer is available here: [**flm-setup.exe**](https://github.com/FastFlowLM/FastFlowLM/releases/latest/download/flm-setup.exe). For more details, see the [release notes](https://github.com/FastFlowLM/FastFlowLM/releases/).

üì∫ [**Watch the quick start video**](https://www.youtube.com/watch?v=mYOfDNkyBII)

> ‚ö†Ô∏è Ensure NPU driver is **32.0.203.258** or later (check via Task Manager‚ÜíPerformance‚ÜíNPU or Device Manager) ‚Äî [Driver Download](https://www.amd.com/en/support).  
> ‚öôÔ∏è **Tip:** Upgrade to the new NPU Driver **32.0.203.304** for over 5‚Äì10% speed boost across all models and context lengths. [Download and Install](https://ryzenai.docs.amd.com/en/latest/inst.html#install-npu-drivers) *(AMD account required)*  

After installation, open **PowerShell** (`Win + X ‚Üí I`). To run a model in terminal (**CLI Mode**):
```powershell
flm run llama3.2:1b
```
> **Notes:**
> - Internet access to HuggingFace is required to download the optimized model kernels.
> - Sometimes downloads from HuggingFace may get corrupted. If this happens, run `flm pull <model_tag> --force` (e.g. `flm pull llama3.2:1b --force`) to re-download and fix them.
> - By default, models are stored in: `C:\Users\<USER>\Documents\flm\models\`  
> - During installation, you can select a different base folder (e.g., if you choose `C:\Users\<USER>\flm`, models will be saved under `C:\Users\<USER>\flm\models\`).  
> - ‚ö†Ô∏è If HuggingFace is not accessible in your region, manually download the model ([check this issue](https://github.com/FastFlowLM/FastFlowLM/issues/2)) and place it in the chosen directory.   

üéâüöÄ FastFlowLM (FLM) is ready ‚Äî your NPU is unlocked and you can start chatting with models right away!

Open **Task Manager** (`Ctrl + Shift + Esc`). Go to the **Performance** tab ‚Üí click **NPU** to monitor usage.  

> **‚ö° Quick Tips:**  
> - Use `/verbose` during a session to turn on performance reporting (toggle off with `/verbose` again).   
> - Type `/bye` to exit a conversation.  
> - Run `flm list` in PowerShell to show all available models.  

To start the local server (**Server Mode**):
```powershell
flm serve llama3.2:1b
```
> The model tag (e.g., `llama3.2:1b`) sets the initial model, which is optional. If another model is requested, FastFlowLM will automatically switch to it. Local server is on port 52625 (default).  

**[![FastFlowLM Docs](https://img.shields.io/badge/FastFlowLM-Detailed%20Instructions-red?style=flat&logo=readthedocs)](https://docs.fastflowlm.com/instructions/)**

---

## üì∞ In the News

- 10/01/2025 üéâ FLM was integrated into AMD's **[Lemonade Server](https://lemonade-server.ai/)** üçã. Watch this **[short demo](https://www.youtube.com/watch?v=w0Tb3h4WUnE)** about using FLM in Lemonade.

---

## üß† Local AI on NPU

FLM makes it easy to run cutting-edge **LLMs** (and now **VLMs**) locally with:
- ‚ö° Fast and low power
- üß∞ Simple CLI and API (REST and OpenAI API)
- üîê Fully private and offline

No model rewrites, no tuning ‚Äî it just works.

---

## ‚úÖ Highlights

- **Runs fully on AMD Ryzen‚Ñ¢ AI NPU** ‚Äî no GPU or CPU load
- **Lightweight runtime (14 MB)** ‚Äî installs within **20 seconds**, easy to integrate    
- **Developer-first flow** ‚Äî like Ollama, but optimized for NPU  
- **Support for long context windows** ‚Äî up to 256k tokens (e.g., Qwen3-4B-Thinking-2507)  
- **No low-level tuning required** ‚Äî You focus on your app, we handle the rest

---

## üìÑ License

- All orchestration code and CLI tools are open-source under the [MIT License](./LICENSE_RUNTIME.txt).
- NPU-accelerated kernels are **proprietary binaries**, free for **non-commercial use** only ‚Äî see [LICENSE_BINARY.txt](./LICENSE_BINARY.txt) and [TERMS.md](./TERMS.md) for details.
- **Non-commercial users:** Please acknowledge FastFlowLM in your README/project page:
  ```
  Powered by [FastFlowLM](https://github.com/FastFlowLM/FastFlowLM)
  ```
For commercial use or licensing inquiries, email us: info@fastflowlm.com

---

üí¨ Have **feedback/issues** or want **early access** to our new releases? [Open an issue](https://github.com/fastflowlm/fastflowlm/issues/new) or [Join our Discord community](https://discord.gg/z24t23HsHF)

---

## üôè Acknowledgements

- Powered by the advanced **AMD Ryzen‚Ñ¢ AI NPU architecture**
- Inspired by the widely adopted [Ollama](https://github.com/ollama/ollama)
- Tokenization accelerated with [MLC-ai/tokenizers-cpp](https://github.com/mlc-ai/tokenizers-cpp)
- Chat formatting via [Google/minja](https://github.com/google/minja)
- Low-level kernels optimized using the powerful [IRON](https://github.com/Xilinx/mlir-aie/tree/main/programming_guide)+[AIE-MLIR](https://github.com/Xilinx/mlir-aie/tree/main)
