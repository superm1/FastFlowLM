---
layout: docs
title: Qwen
nav_order: 3
parent: Models
---

## ğŸ§© Model Card: [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)

- **Type:** Text-to-Text
- **Think:** Toggleable
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)
- **Quantization:** Q4_1
- **Max Context Length:** 32k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3:0.6b
```

ğŸ“ **Note:**

- **CLI**: Type `/think` to toggle on/off interactively.  
- **Server Mode**: Set the `"think"` flag in the request payload.

---

## ğŸ§© Model Card: [Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B)

- **Type:** Text-to-Text
- **Think:** Toggleable
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B)
- **Quantization:** Q4_1
- **Max Context Length:** 32k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3:0.6b
```

ğŸ“ **Note:**

- **CLI**: Type `/think` to toggle on/off interactively.  
- **Server Mode**: Set the `"think"` flag in the request payload.

---

## ğŸ§© Model Card: [Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B)

- **Type:** Text-to-Text
- **Think:** Toggleable
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B)
- **Quantization:** Q4_1
- **Max Context Length:** 32k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3:4b
```

ğŸ“ **Note:**

- **CLI**: Type `/think` to toggle on/off interactively.  
- **Server Mode**: Set the `"think"` flag in the request payload.

---

## ğŸ§© Model Card: [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B)

- **Type:** Text-to-Text
- **Think:** Toggleable
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B)
- **Quantization:** Q4_1
- **Max Context Length:** 32k tokens  
- **Default Context Length:** 16k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3:8b
```

ğŸ“ **Note:**

- **CLI**: Type `/think` to toggle on/off interactively.  
- **Server Mode**: Set the `"think"` flag in the request payload.

---

## ğŸ§© Model Card: [Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507)

- **Type:** Text-to-Text
- **Think:** Yes
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507)
- **Quantization:** Q4_1
- **Max Context Length:** 256k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3-tk:4b
```

---

## ğŸ§© Model Card: [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)

- **Type:** Text-to-Text
- **Think:** No
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)
- **Quantization:** Q4_1
- **Max Context Length:** 256k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3-it:4b
```

---

## ğŸ§© Model Card: [Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)

- **Type:** Image-Text-to-Text
- **Think:** No
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)
- **Quantization:** Q4_1
- **Max Context Length:** 256k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen3vl-it:4b
```

â–¶ï¸ Image Resize Options

You can control image resizing when running or serving the model using the `--resize` flag:

```shell
flm run qwen3vl-it:3b --resize 1
```

```shell
flm serve qwen3vl-it:3b --resize 1
```

The `--resize` option determines the target length of the imageâ€™s **longest** side:

- -1 â€” Disable resizing (use the original image)
- 0 â€” Resize so the longest side is **1080 px** (Default)
- 1 â€” Resize so the longest side is **1920 px**
- 2 â€” Resize so the longest side is **2560 px**

> Don't worryâ€”if your image is already smaller than 1080px, it keeps its original resolution! âœ¨

ğŸ“ **Note**

- Image understanding adapts to image size. Image TTFT can range from under 1 second to ~200 seconds depending on resolution. Use lower-resolution images (720p or below) unless high resolution is required (e.g. OCR on small text).
- Video understanding is not supported yet.

---

## ğŸ§© Model Card: [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)

- **Type:** Text-to-Text
- **Think:** No
- **Tool Calling Support:** No  
- **Base Model:** [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
- **Quantization:** Q4_1
- **Max Context Length:** 32k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen2.5-it:3b
```

---

## ğŸ§© Model Card: [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)

- **Type:** Image-Text-to-Text
- **Think:** No
- **Tool Calling Support:** Yes  
- **Base Model:** [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- **Quantization:** Q4_1
- **Max Context Length:** 256k tokens  
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/ï¸#-set-context-length-at-launch)**

â–¶ï¸ Run with FastFlowLM in PowerShell:  

```shell
flm run qwen2.5vl-it:3b
```

â–¶ï¸ Image Resize Options

You can control image resizing when running or serving the model using the `--resize` flag:

```shell
flm run qwen2.5vl-it:3b --resize 1
```

```shell
flm serve qwen2.5vl-it:3b --resize 1
```

The `--resize` option determines the target length of the imageâ€™s **longest** side:

- -1 â€” Disable resizing (use the original image)
- 0 â€” Resize so the longest side is **1080 px** (Default)
- 1 â€” Resize so the longest side is **1920 px**
- 2 â€” Resize so the longest side is **2560 px**

> Don't worryâ€”if your image is already smaller than 1080px, it keeps its original resolution! âœ¨

ğŸ“ **Note**

- Image understanding adapts to image size. Image TTFT can range from under 1 second to ~200 seconds depending on resolution. Use lower-resolution images (720p or below) unless high resolution is required (e.g. OCR on small text).
- Video understanding is not supported yet.