---
layout: docs
title: LFM
nav_order: 7
parent: Models
---

## üß© Model Card: [LiquidAI/LFM2-1.2B](https://huggingface.co/LiquidAI/LFM2-1.2B)

- **Type:** Text-to-Text
- **Think:** No
- **Base Model:** [LiquidAI/LFM2-1.2B](https://huggingface.co/LiquidAI/LFM2-1.2B)
- **Quantization:** Q4_0
- **Max Context Length:** 32k tokens 
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/#-set-context-length-at-launch)**

‚ñ∂Ô∏è Run with FastFlowLM in PowerShell:  

```shell
flm run lfm2:1.2b
```

---

## üß© Model Card: [LiquidAI/LFM2-2.6B](https://huggingface.co/LiquidAI/LFM2-2.6B)

- **Type:** Text-to-Text
- **Think:** No
- **Base Model:** [LiquidAI/LFM2-2.6B](https://huggingface.co/LiquidAI/LFM2-2.6B)
- **Quantization:** Q4_0
- **Max Context Length:** 32k tokens 
- **Default Context Length:** 32k tokens ([change default](https://fastflowlm.com/docs/instructions/cli/#-change-default-context-length-max))  
- **[Set Context Length at Launch](https://fastflowlm.com/docs/instructions/cli/#-set-context-length-at-launch)**

‚ñ∂Ô∏è Run with FastFlowLM in PowerShell:  

```shell
flm run lfm2:2.6b
```

---