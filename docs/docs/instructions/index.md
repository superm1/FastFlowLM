---
layout: docs
title: Instructions
nav_order: 2
has_children: true
---

# ğŸ› ï¸ Instructions

**FastFlowLM (FLM)** is a deeply optimized runtime for **local LLM inference on AMD NPUs** â€”  
ultra-fast, power-efficient, and 100% offline.

Its user interface and workflow are similar to **Ollama**, but purpose-built for AMD's XDNA architecture.

This section will walk you through how to use FastFlowLM with examples.

---

## ğŸ“š Sections

- [System Command and CLI Mode](cli/)
- [Server Mode](server/)
- [Server Basics](server/basics/)
- [API / Client Usage](server/openapi/)
- [Open WebUI](server/webui/)
- [LangChain RAG](server/rag_LangChain/)
- [LangChain Web Search](server/websearch_LangChain/)
- [Obsidian](server/obsidian/)
- [Microsoft AI Toolkit](server/msft_AI_toolkit/)