---
layout: docs
title: Get Started
nav_order: 1
has_children: false
---

## âš™ï¸ System Requirements

- ðŸ§  **Memory:** 32â€¯GB RAM or higher recommended  
- âš¡ **CPU/NPU:** AMD Ryzenâ„¢ AI laptop with XDNA2 NPU  
- ðŸ–¥ï¸ **OS:** Windows 11

> While FastFlowLM can run with 16â€¯GB RAM, complex models (e.g., 3B or 8B) may require >= 32â€¯GB for optimal performance and longer context length (more kv cache).

---

## ðŸš¨ CRITICAL: NPU Driver Requirement

You must have AMD NPU driver **version number >= 32.0.203.304** (`.304` is the minimum requirement but `.311` is recommended) installed for FastFlowLM to work correctly.

<!-- > âš™ï¸ **Tip:** Upgrade to the new NPU Driver **32.0.203.304** for over 5â€“10% speed boost across all models and context lengths. [Download and Install](https://ryzenai.docs.amd.com/en/latest/inst.html#install-npu-drivers) *(AMD account required)*   -->
- Check via:  
  **Task Manager â†’ Performance â†’ NPU**  
  or  
  **Device Manager â†’ NPU**

ðŸ”— [Download AMD Driver](https://www.amd.com/en/support)

---

## ðŸ’¾ Install FastFlowLM (Windows)

A packaged FLM Windows installer is available here:  
[**flm-setup.exe**](https://github.com/FastFlowLM/FastFlowLM/releases/latest/download/flm-setup.exe)

> To install silently, open PowerShell, navigate (`cd`) to the download folder, and run:

```shell
.\flm-setup.exe /Silent
# or
.\flm-setup.exe /VERYSilent
```

If you see **"Windows protected your PC"**, click **More info**, then select **Run anyway**.

ðŸ“º [**Watch the quick start video**](https://www.youtube.com/watch?v=mYOfDNkyBII)

For version history and changelog, see the [release notes](https://github.com/FastFlowLM/FastFlowLM/releases/).

---

## ðŸš€ NPU Power Mode

By default, **FLM runs in `performance` NPU power mode**. You can switch to other NPU power modes (`powersaver`, `balanced`, or `turbo`) using the `--pmode` flag:

**CLI mode:**
```shell
flm run gemma3:4b --pmode balanced
```

**Server mode:**
```shell
flm serve gemma3:4b --pmode balanced
```

> âš ï¸ Note: Using powersaver or balanced will lower NPU clock speeds and cause a significant drop in speed. For more details about NPU power mode, refer to the [AMD XRT SMI Documentation](https://ryzenai.docs.amd.com/en/latest/xrt_smi.html).

---

## ðŸ§ª Quick Test (CLI Mode)

After installation, do a quick test to see if FastFlowLM is properly installed. Open **PowerShell** (`Win + X â†’ I`), and run a model in terminal (CLI mode):

```shell
flm run llama3.2:1b
```

> **Notes:**
> - Internet access to HuggingFace is required to download the optimized model kernels.  
> - By default, models are stored in: `C:\Users\<USER>\flm\models\`  
> - During installation, you can select a different base folder (e.g., if you choose `C:\Users\<USER>\Documents\flm`, models will be saved under `C:\Users\<USER>\Documents\flm\models\`).
> - âš ï¸ If HuggingFace is not directly accessible in your region, you can manually download the model (e.g., [hf-mirror](https://hf-mirror.com/models?search=fastflowlm)) and place it in the directory.

ðŸŽ‰ðŸš€ FastFlowLM (FLM) is ready â€” your NPU is unlocked and you can start chatting with models right away!

Open **Task Manager** (`Ctrl + Shift + Esc`). Go to the **Performance** tab â†’ click **NPU** to monitor usage.  

> **âš¡ Quick Tips:**  
> - Use `/verbose` during a session to turn on performance reporting (toggle off with `/verbose` again).   
> - Type `/bye` to exit a conversation.  
> - Run `flm list` in PowerShell to show all available models.  
