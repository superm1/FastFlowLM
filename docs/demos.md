---
layout: page
title: "Demos"
permalink: /demos/
description: "Interactive experiences that showcase FastFlowLM + Ryzen AI performance."
sections:
  - type: hero
    kicker: "Demos"
    title: "See FastFlowLM running on real hardware"
    body: |
      Explore live demos and recordings that showcase FastFlowLM powering LLMs, VLMs, and embeddings fully on the
      Ryzen™ AI NPU. From interactive chat experiences to system integrations, these examples highlight what’s possible
      when you make the NPU the primary inference engine.
    ctas:
      - label: "Watch on YouTube"
        href: "https://www.youtube.com/@FastFlowLM-YT/playlists"
        style: primary
        external: true
      - label: "Remote test drive"
        href: "/test-drive/"
        style: ghost

  - type: media
    variant: alt
    kicker: "GPT-OSS on NPU"
    title: "GPT-OSS-20B streaming fully on the Ryzen™ AI NPU"
    body: |
      Stream GPT-OSS-20B locally with FastFlowLM, keeping CPU and GPU usage minimal while the NPU does the heavy lifting.
    media:
      src: "/assets/gpt-oss-demo.gif"
      alt: "GPT-OSS 20B running locally on the Ryzen AI NPU"
      href: "https://youtu.be/sZt1WyNoL2U?si=7U3z6u6E9KF6G_Dd"
      kicker: "GPT-OSS on NPU"
      body: |
        Run large-scale chat and reasoning workloads directly on your Ryzen™ AI NPU with stable, low-power performance—
        ideal for long-running sessions and experimentation.

  - type: media
    variant: alt
    kicker: "Gemma3 (Vision) on NPU"
    title: "Gemma3 (Vision) understand and describe the image"
    body: |
      Gemma3 Vision turns your NPU into a visual reasoning engine, delivering instant understanding with near-zero CPU/GPU load.
    media:
      src: "/assets/gemma3_4b.gif"
      alt: "Gemma3 vision model understanding an image"
      href: "https://youtu.be/BLd2j9dPw40?si=5GzOW2FlZgGVp8Xi"
      kicker: "Gemma3 (Vision) on NPU"
      body: |
        Understand and describe images instantly — FastFlowLM runs Google Gemma3 fully on the NPU for fast, private, and efficient vision inference.

  - type: media
    variant: alt
    kicker: "Whisper on-device"
    title: "Transcribe and summarize long-form audio locally"
    body: |
      Use FastFlowLM to run Whisper completely on the NPU, keeping voice and meeting data on your device while you
      transcribe and summarize hours of audio.
    media:
      src: "/assets/cramer2.gif"
      alt: "Whisper transcription and summarization demo"
      href: "https://youtu.be/0t8ijUPg4A0?si=ETKdvig6lYiZb1Q_"
      kicker: "Whisper on-device"
      body: |
        Turn recordings into searchable transcripts and concise summaries with NPU-accelerated Whisper,
        without sending audio to the cloud.

  - type: media
    variant: alt
    kicker: "Llama 3.2 on Open WebUI"
    title: "Interact with Llama 3.2-3B through the FastFlowLM WebUI"
    body: |
      Chat with Llama 3.2-3B in a browser-based UI powered by FastFlowLM and Open WebUI, with responses served directly from the
      Ryzen™ AI NPU.
    media:
      src: "/assets/llama-demo.gif"
      alt: "Llama 3.2 chat demo running in the FastFlowLM WebUI"
      href: "https://youtu.be/mPrr9FLd8ps?si=vsyHkmtrBjP4s-dq"
      kicker: "Llama 3.2 on Open WebUI"
      body: |
        Explore NPU-backed chat experiences with a modern WebUI, suitable for local copilots, assistants,
        and prototyping interactive products.
---


